{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerai Tournament\n",
    "## Data Download and Test Submissions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download Dependencies and Create Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import gc \n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path \n",
    "\n",
    "# import Numerai API\n",
    "from numerapi import NumerAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save predictions with given name\n",
    "def save_prediction(df, name):\n",
    "    try: \n",
    "        Path('prediction_files').mkdir(exist_ok=True, parents=True)\n",
    "    except Exception as ex:\n",
    "        pass\n",
    "    df.to_csv(f'prediction_files/{name}.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save model with given name\n",
    "def save_model(model, name):\n",
    "    try:\n",
    "        Path('models').mkdir(exist_ok=True, parents=True)\n",
    "    except Exception as ex:\n",
    "        pass\n",
    "    pd.to_pickle(model, f'models/{name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load model, given name\n",
    "def load_model(name):\n",
    "    path = Path(f'models/{name}.pkl')\n",
    "    if path.is_file():\n",
    "        model = pd.read_pickle(f'models/{name}.pkl')\n",
    "    else:\n",
    "        model = False \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save model configs\n",
    "def save_model_config(model_config, model_name):\n",
    "    try:\n",
    "        Path('model_configs').mkdir(exist_ok=True, parents=True)\n",
    "    except Exception as ex:\n",
    "        pass \n",
    "    with open(f'model_configs/{model_name}.json', 'w') as fp:\n",
    "        json.dump(model_config, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load model configs\n",
    "def load_model_config(model_name):\n",
    "    path_str = f'model_configs/{model_name}.json'\n",
    "    path = Path(path_str)\n",
    "    if path.is_file():\n",
    "        with open(path_str, 'r') as fp:\n",
    "            model_config = json.load(fp)\n",
    "    else:\n",
    "        model_config = False\n",
    "    return model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_biggest_change_features(corrs, n):\n",
    "    all_eras = corrs.index.sort_values()\n",
    "    h1_eras = all_eras[: len(all_eras) // 2]\n",
    "    h2_eras = all_eras[len(all_eras) // 2 :]\n",
    "\n",
    "    h1_corr_means = corrs.loc[h1_eras, :].mean()\n",
    "    h2_corr_means = corrs.loc[h2_eras, :].mean()\n",
    "\n",
    "    corr_diffs = h2_corr_means - h1_corr_means\n",
    "    worst_n = corr_diffs.abs().sort_values(ascending=False).head(n).index.tolist()\n",
    "    return worst_n\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_series_cross_val_splits(data, cv=3, embargo=12):\n",
    "    all_train_eras = data[ERA_COL].unique()\n",
    "    len_split = len(all_train_eras) // cv\n",
    "    test_splits = [\n",
    "        all_train_eras[i * len_split : (i + 1) * len_split] for i in range(cv)\n",
    "    ]\n",
    "    # fix the last test split to have all the last eras, in case the number of eras wasn't divisible by cv\n",
    "    remainder = len(all_train_eras) % cv\n",
    "    if remainder != 0:\n",
    "        test_splits[-1] = np.append(test_splits[-1], all_train_eras[-remainder:])\n",
    "\n",
    "    train_splits = []\n",
    "    for test_split in test_splits:\n",
    "        test_split_max = int(np.max(test_split))\n",
    "        test_split_min = int(np.min(test_split))\n",
    "        # get all of the eras that aren't in the test split\n",
    "        train_split_not_embargoed = [\n",
    "            e\n",
    "            for e in all_train_eras\n",
    "            if not (test_split_min <= int(e) <= test_split_max)\n",
    "        ]\n",
    "        # embargo the train split so we have no leakage.\n",
    "        # one era is length 5, so we need to embargo by target_length/5 eras.\n",
    "        # To be consistent for all targets, let's embargo everything by 60/5 == 12 eras.\n",
    "        train_split = [\n",
    "            e\n",
    "            for e in train_split_not_embargoed\n",
    "            if abs(int(e) - test_split_max) > embargo\n",
    "            and abs(int(e) - test_split_min) > embargo\n",
    "        ]\n",
    "        train_splits.append(train_split)\n",
    "\n",
    "    # convenient way to iterate over train and test splits\n",
    "    train_test_zip = zip(train_splits, test_splits)\n",
    "    return train_test_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neutralize(\n",
    "    df, columns, neutralizers=None, proportion=1.0, normalize=True, era_col=\"era\", verbose=False\n",
    "):\n",
    "    if neutralizers is None:\n",
    "        neutralizers = []\n",
    "    unique_eras = df[era_col].unique()\n",
    "    computed = []\n",
    "    if verbose:\n",
    "        iterator = tqdm(unique_eras)\n",
    "    else:\n",
    "        iterator = unique_eras\n",
    "    for u in iterator:\n",
    "        df_era = df[df[era_col] == u]\n",
    "        scores = df_era[columns].values\n",
    "        if normalize:\n",
    "            scores2 = []\n",
    "            for x in scores.T:\n",
    "                x = (scipy.stats.rankdata(x, method=\"ordinal\") - 0.5) / len(x)\n",
    "                x = scipy.stats.norm.ppf(x)\n",
    "                scores2.append(x)\n",
    "            scores = np.array(scores2).T\n",
    "        exposures = df_era[neutralizers].values\n",
    "\n",
    "        scores -= proportion * exposures.dot(\n",
    "            np.linalg.pinv(exposures.astype(np.float32), rcond=1e-6).dot(\n",
    "                scores.astype(np.float32)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        scores /= scores.std(ddof=0)\n",
    "\n",
    "        computed.append(scores)\n",
    "\n",
    "    return pd.DataFrame(np.concatenate(computed), columns=columns, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neutralize_series(series, by, proportion=1.0):\n",
    "    scores = series.values.reshape(-1, 1)\n",
    "    exposures = by.values.reshape(-1, 1)\n",
    "\n",
    "    # this line makes series neutral to a constant column so that it's centered and for sure gets corr 0 with exposures\n",
    "    exposures = np.hstack(\n",
    "        (exposures, np.array([np.mean(series)] * len(exposures)).reshape(-1, 1))\n",
    "    )\n",
    "\n",
    "    correction = proportion * (\n",
    "        exposures.dot(np.linalg.lstsq(exposures, scores, rcond=None)[0])\n",
    "    )\n",
    "    corrected_scores = scores - correction\n",
    "    neutralized = pd.Series(corrected_scores.ravel(), index=series.index)\n",
    "    return neutralized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unif(df):\n",
    "    x = (df.rank(method=\"first\") - 0.5) / len(df)\n",
    "    return pd.Series(x, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_neutral_mean(\n",
    "    df, prediction_col, target_col, features_for_neutralization=None\n",
    "):\n",
    "    if features_for_neutralization is None:\n",
    "        features_for_neutralization = [c for c in df.columns if c.startswith(\"feature\")]\n",
    "    df.loc[:, \"neutral_sub\"] = neutralize(\n",
    "        df, [prediction_col], features_for_neutralization\n",
    "    )[prediction_col]\n",
    "    scores = (\n",
    "        df.groupby(\"era\")\n",
    "        .apply(lambda x: (unif(x[\"neutral_sub\"]).corr(x[target_col])))\n",
    "        .mean()\n",
    "    )\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_neutral_mean_tb_era(\n",
    "    df, prediction_col, target_col, tb, features_for_neutralization=None\n",
    "):\n",
    "    if features_for_neutralization is None:\n",
    "        features_for_neutralization = [c for c in df.columns if c.startswith(\"feature\")]\n",
    "    temp_df = df.reset_index(\n",
    "        drop=True\n",
    "    ).copy()  # Reset index due to use of argsort later\n",
    "    temp_df.loc[:, \"neutral_sub\"] = neutralize(\n",
    "        temp_df, [prediction_col], features_for_neutralization\n",
    "    )[prediction_col]\n",
    "    temp_df_argsort = temp_df.loc[:, \"neutral_sub\"].argsort()\n",
    "    temp_df_tb_idx = pd.concat([temp_df_argsort.iloc[:tb], temp_df_argsort.iloc[-tb:]])\n",
    "    temp_df_tb = temp_df.loc[temp_df_tb_idx]\n",
    "    tb_fnc = unif(temp_df_tb[\"neutral_sub\"]).corr(temp_df_tb[target_col])\n",
    "    return tb_fnc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_score_by_date(df, columns, target, tb=None, era_col=\"era\"):\n",
    "    unique_eras = df[era_col].unique()\n",
    "    computed = []\n",
    "    for u in unique_eras:\n",
    "        df_era = df[df[era_col] == u]\n",
    "        era_pred = np.float64(df_era[columns].values.T)\n",
    "        era_target = np.float64(df_era[target].values.T)\n",
    "\n",
    "        if tb is None:\n",
    "            ccs = np.corrcoef(era_target, era_pred)[0, 1:]\n",
    "        else:\n",
    "            tbidx = np.argsort(era_pred, axis=1)\n",
    "            tbidx = np.concatenate([tbidx[:, :tb], tbidx[:, -tb:]], axis=1)\n",
    "            ccs = [\n",
    "                np.corrcoef(era_target[tmpidx], tmppred[tmpidx])[0, 1]\n",
    "                for tmpidx, tmppred in zip(tbidx, era_pred)\n",
    "            ]\n",
    "            ccs = np.array(ccs)\n",
    "\n",
    "        computed.append(ccs)\n",
    "\n",
    "    return pd.DataFrame(np.array(computed), columns=columns, index=df[era_col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exposure_dissimilarity_per_era(df, prediction_col, example_col, feature_cols=None):\n",
    "    if feature_cols is None:\n",
    "        feature_cols = [c for c in df.columns if c.startswith(\"feature\")]\n",
    "    u = df.loc[:, feature_cols].corrwith(df[prediction_col])\n",
    "    e = df.loc[:, feature_cols].corrwith(df[example_col])\n",
    "    return 1 - (np.dot(u, e) / np.dot(e, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_metrics(\n",
    "    validation_data,\n",
    "    pred_cols,\n",
    "    example_col,\n",
    "    fast_mode=False,\n",
    "    target_col='target_nomi_v4_20',\n",
    "    features_for_neutralization=None,\n",
    "):\n",
    "    validation_stats = pd.DataFrame()\n",
    "    feature_cols = [c for c in validation_data if c.startswith(\"feature_\")]\n",
    "    for pred_col in pred_cols:\n",
    "        # Check the per-era correlations on the validation set (out of sample)\n",
    "        validation_correlations = validation_data.groupby(ERA_COL).apply(\n",
    "            lambda d: unif(d[pred_col]).corr(d[target_col])\n",
    "        )\n",
    "\n",
    "        mean = validation_correlations.mean()\n",
    "        std = validation_correlations.std(ddof=0)\n",
    "        sharpe = mean / std\n",
    "\n",
    "        validation_stats.loc[\"mean\", pred_col] = mean\n",
    "        validation_stats.loc[\"std\", pred_col] = std\n",
    "        validation_stats.loc[\"sharpe\", pred_col] = sharpe\n",
    "\n",
    "        rolling_max = (\n",
    "            (validation_correlations + 1)\n",
    "            .cumprod()\n",
    "            .rolling(window=9000, min_periods=1)  # arbitrarily large\n",
    "            .max()\n",
    "        )\n",
    "        daily_value = (validation_correlations + 1).cumprod()\n",
    "        max_drawdown = -((rolling_max - daily_value) / rolling_max).max()\n",
    "        validation_stats.loc[\"max_drawdown\", pred_col] = max_drawdown\n",
    "\n",
    "        payout_scores = validation_correlations.clip(-0.25, 0.25)\n",
    "        payout_daily_value = (payout_scores + 1).cumprod()\n",
    "\n",
    "        apy = (\n",
    "            ((payout_daily_value.dropna().iloc[-1]) ** (1 / len(payout_scores)))\n",
    "            ** 49  # 52 weeks of compounding minus 3 for stake compounding lag\n",
    "            - 1\n",
    "        ) * 100\n",
    "\n",
    "        validation_stats.loc[\"apy\", pred_col] = apy\n",
    "\n",
    "        if not fast_mode:\n",
    "            # Check the feature exposure of your validation predictions\n",
    "            max_per_era = validation_data.groupby(ERA_COL).apply(\n",
    "                lambda d: d[feature_cols].corrwith(d[pred_col]).abs().max()\n",
    "            )\n",
    "            max_feature_exposure = max_per_era.mean()\n",
    "            validation_stats.loc[\n",
    "                \"max_feature_exposure\", pred_col\n",
    "            ] = max_feature_exposure\n",
    "\n",
    "            # Check feature neutral mean\n",
    "            feature_neutral_mean = get_feature_neutral_mean(\n",
    "                validation_data, pred_col, target_col, features_for_neutralization\n",
    "            )\n",
    "            validation_stats.loc[\n",
    "                \"feature_neutral_mean\", pred_col\n",
    "            ] = feature_neutral_mean\n",
    "\n",
    "            # Check TB200 feature neutral mean\n",
    "            tb200_feature_neutral_mean_era = validation_data.groupby(ERA_COL).apply(\n",
    "                lambda df: get_feature_neutral_mean_tb_era(\n",
    "                    df, pred_col, target_col, 200, features_for_neutralization\n",
    "                )\n",
    "            )\n",
    "            validation_stats.loc[\n",
    "                \"tb200_feature_neutral_mean\", pred_col\n",
    "            ] = tb200_feature_neutral_mean_era.mean()\n",
    "\n",
    "            # Check top and bottom 200 metrics (TB200)\n",
    "            tb200_validation_correlations = fast_score_by_date(\n",
    "                validation_data, [pred_col], target_col, tb=200, era_col=ERA_COL\n",
    "            )\n",
    "\n",
    "            tb200_mean = tb200_validation_correlations.mean()[pred_col]\n",
    "            tb200_std = tb200_validation_correlations.std(ddof=0)[pred_col]\n",
    "            tb200_sharpe = tb200_mean / tb200_std\n",
    "\n",
    "            validation_stats.loc[\"tb200_mean\", pred_col] = tb200_mean\n",
    "            validation_stats.loc[\"tb200_std\", pred_col] = tb200_std\n",
    "            validation_stats.loc[\"tb200_sharpe\", pred_col] = tb200_sharpe\n",
    "\n",
    "        # MMC over validation\n",
    "        mmc_scores = []\n",
    "        corr_scores = []\n",
    "        for _, x in validation_data.groupby(ERA_COL):\n",
    "            series = neutralize_series(unif(x[pred_col]), (x[example_col]))\n",
    "            mmc_scores.append(np.cov(series, x[target_col])[0, 1] / (0.29**2))\n",
    "            corr_scores.append(unif(x[pred_col]).corr(x[target_col]))\n",
    "\n",
    "        val_mmc_mean = np.mean(mmc_scores)\n",
    "        val_mmc_std = np.std(mmc_scores)\n",
    "        corr_plus_mmcs = [c + m for c, m in zip(corr_scores, mmc_scores)]\n",
    "        corr_plus_mmc_sharpe = np.mean(corr_plus_mmcs) / np.std(corr_plus_mmcs)\n",
    "\n",
    "        validation_stats.loc[\"mmc_mean\", pred_col] = val_mmc_mean\n",
    "        validation_stats.loc[\"corr_plus_mmc_sharpe\", pred_col] = corr_plus_mmc_sharpe\n",
    "\n",
    "        # Check correlation with example predictions\n",
    "        per_era_corrs = validation_data.groupby(ERA_COL).apply(\n",
    "            lambda d: unif(d[pred_col]).corr(unif(d[example_col]))\n",
    "        )\n",
    "        corr_with_example_preds = per_era_corrs.mean()\n",
    "        validation_stats.loc[\n",
    "            \"corr_with_example_preds\", pred_col\n",
    "        ] = corr_with_example_preds\n",
    "\n",
    "        # Check exposure dissimilarity per era\n",
    "        tdf = validation_data.groupby(ERA_COL).apply(\n",
    "            lambda df: exposure_dissimilarity_per_era(\n",
    "                df, pred_col, example_col, feature_cols\n",
    "            )\n",
    "        )\n",
    "        validation_stats.loc[\"exposure_dissimilarity_mean\", pred_col] = tdf.mean()\n",
    "\n",
    "    # .transpose so that stats are columns and the model_name is the row\n",
    "    return validation_stats.transpose()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Download Tournament Data and Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull in api credentials \n",
    "with open('/Users/akg/.secret/numerai/numerai-keys.json', 'r') as f:\n",
    "    creds = json.load(f)\n",
    "\n",
    "public_key = creds['public-key']\n",
    "secret_key = creds['secret-key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current round: 416\n"
     ]
    }
   ],
   "source": [
    "# instantiate api object\n",
    "napi = NumerAPI(public_key, secret_key)\n",
    "\n",
    "# get and print current round\n",
    "current_round = napi.get_current_round()\n",
    "print(f'Current round: {current_round}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tournament data changes every week so we specify the round in the name. \n",
    "\n",
    "Training and validation data only change periodically, so no need to download them every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 13:55:54,539 INFO numerapi.utils: starting download\n",
      "v4.1/train.parquet: 1.45GB [04:41, 5.14MB/s]                            \n",
      "2023-02-08 14:00:36,937 INFO numerapi.utils: starting download\n",
      "v4.1/validation.parquet: 1.51GB [04:56, 5.10MB/s]                             \n",
      "2023-02-08 14:05:34,744 INFO numerapi.utils: starting download\n",
      "v4.1/live_416.parquet: 4.52MB [00:00, 4.72MB/s]                            \n",
      "2023-02-08 14:05:36,528 INFO numerapi.utils: starting download\n",
      "v4.1/validation_example_preds.parquet: 57.1MB [00:09, 6.22MB/s]                            \n",
      "2023-02-08 14:05:46,424 INFO numerapi.utils: starting download\n",
      "v4.1/features.json: 703kB [00:00, 1.39MB/s]                           \n"
     ]
    }
   ],
   "source": [
    "# specify path to save dataset files \n",
    "Path('./v4.1').mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "# download various files \n",
    "napi.download_dataset('v4.1/train.parquet')\n",
    "napi.download_dataset('v4.1/validation.parquet')\n",
    "napi.download_dataset('v4.1/live.parquet', f'v4.1/live_{current_round}.parquet')\n",
    "napi.download_dataset('v4.1/validation_example_preds.parquet')\n",
    "napi.download_dataset('v4.1/features.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prediction(df, name):\n",
    "    try: \n",
    "        Path('prediction_files').mkdir(exist_ok=True, parents=True)\n",
    "    except Exception as ex:\n",
    "        pass\n",
    "    df.to_csv(f'prediction_files/{name}.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "numerai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e11cfa367660235408dd5dba0005059e205bf138e176e00c30571fc276fa0138"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
